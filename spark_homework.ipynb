{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322b04a0",
   "metadata": {},
   "source": [
    "Digital Skola, Batch 11\n",
    "\n",
    "Spark HomeWork\n",
    "\n",
    "M. Abdurrahman Shidiq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f4065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e7c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 14:29:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/20 14:29:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Create a spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5a5a0",
   "metadata": {},
   "source": [
    "# Extract\n",
    " Extract process: read data tsb ke dalam csv dan dijadikan dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91a12a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our inferred schema:\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"fhv_tripdata_2019-01.csv.gz\")\n",
    "print(\"Here is our inferred schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67bda645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00001|2019-01-01 00:30:00|2019-01-01 02:51:55|        null|        null|   null|                B00001|\n",
      "|              B00001|2019-01-01 00:45:00|2019-01-01 00:54:49|        null|        null|   null|                B00001|\n",
      "|              B00001|2019-01-01 00:15:00|2019-01-01 00:54:52|        null|        null|   null|                B00001|\n",
      "|              B00008|2019-01-01 00:19:00|2019-01-01 00:39:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:27:00|2019-01-01 00:37:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:48:00|2019-01-01 01:02:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:50:00|2019-01-01 00:59:00|        null|        null|   null|                B00008|\n",
      "|              B00008|2019-01-01 00:51:00|2019-01-01 00:56:00|        null|        null|   null|                B00008|\n",
      "|              B00009|2019-01-01 00:44:00|2019-01-01 00:58:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:19:00|2019-01-01 00:36:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:36:00|2019-01-01 00:49:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:26:00|2019-01-01 00:32:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:26:00|2019-01-01 00:36:00|        null|        null|   null|                B00009|\n",
      "|              B00009|2019-01-01 00:58:00|2019-01-01 01:05:00|        null|        null|   null|                B00009|\n",
      "|              B00013|2019-01-01 00:02:29|2019-01-02 00:25:30|        null|        null|   null|                B00013|\n",
      "|              B00013|2019-01-01 00:01:33|2019-01-02 00:18:16|        null|        null|   null|                B00013|\n",
      "|              B00037|2019-01-01 00:02:43|2019-01-01 00:10:14|        null|         265|   null|                B00037|\n",
      "|              B00037|2019-01-01 00:26:02|2019-01-01 00:37:00|        null|         265|   null|                B00037|\n",
      "|              B00037|2019-01-01 00:11:16|2019-01-01 00:25:41|        null|         265|   null|                B00037|\n",
      "|              B00037|2019-01-01 00:33:45|2019-01-01 00:45:28|        null|         265|   null|                B00037|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96657aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23143222"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7fbd8",
   "metadata": {},
   "source": [
    "# Transform\n",
    "Transforms process: filter data dimana kolom `PUlocationID` dan `DOlocationID` **tidak boleh kosong** dan `pickup_datetime` mulai dari **1 Jan 2019 sampai 10 Jan 2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c9debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tripData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab86e72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02832|2019-01-02 09:00:00|2019-01-02 09:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 10:00:00|2019-01-02 10:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 10:00:00|2019-01-02 10:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 10:30:00|2019-01-02 11:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 11:00:00|2019-01-02 11:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 11:30:00|2019-01-02 12:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 12:00:00|2019-01-02 12:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 12:30:00|2019-01-02 13:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 13:30:00|2019-01-02 14:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 14:00:00|2019-01-02 14:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 14:00:00|2019-01-02 14:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 15:00:00|2019-01-02 15:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 15:30:00|2019-01-02 16:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 16:00:00|2019-01-02 16:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 17:30:00|2019-01-02 18:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-02 18:30:00|2019-01-02 19:00:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-03 08:00:00|2019-01-03 08:30:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-03 09:00:00|2019-01-03 09:45:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-03 10:00:00|2019-01-03 10:45:00|           0|           0|   null|                B02832|\n",
      "|              B02832|2019-01-03 11:00:00|2019-01-03 11:45:00|           0|           0|   null|                B02832|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    tripData\n",
    "WHERE\n",
    "    PUlocationID IS NOT NULL\n",
    "    AND DOlocationID IS NOT NULL\n",
    "    AND DATE(pickup_datetime) BETWEEN '2019-01-01' AND '2019-01-10'\n",
    "ORDER BY 4,5,2\n",
    "\"\"\"\n",
    "\n",
    "sqlDF = spark.sql(query)\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5160f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6006104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc0be3",
   "metadata": {},
   "source": [
    "# Load\n",
    "Load Process : write data setelah proses transform ke parquet dan json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f3f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 14:32:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# PARQUET\n",
    "sqlDF.write.parquet(\"spark_write_parquet.parquet\")\n",
    "\n",
    "# JSON\n",
    "sqlDF.write.parquet(\"spark_write_json.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d707399e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9123d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
